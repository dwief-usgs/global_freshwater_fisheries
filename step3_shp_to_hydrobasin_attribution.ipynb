{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a placeholder exploring methods to attribute vector data (points/lines/polygons) to HydroBASINS.  Many open source methods are available but finding a method that isn't extremly slow is the challenge.  Each vector data type may have different solutions that are most efficient.  This step needs more exploration, although the point to basin example seems to be working well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get list of level 3 basins\n",
    "for lvl3 basin\n",
    "    if #rows <4500\n",
    "        get level 5 basins:\n",
    "        for lvl 5 basin:\n",
    "            get field and stat info from csv\n",
    "            run stats\n",
    "    else:\n",
    "        run stats\n",
    "        \n",
    "        \n",
    "        in run stats for stat in stats = area_weighted\n",
    "                     for stat in stats == len_weighted\n",
    "                     for stat in stat == max\n",
    "                     for stat in stat == min\n",
    "                     for stat in stat =="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Example Summarizing Point Based Data to Basins\n",
    "      *Point to basin example shown below using GRanD dam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed packages\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin   #note there is a lot of documentation to resolve rtree issues \n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "from utils import file_management as f_mng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First prep data by reading in basin and dam data into geopandas.  Subset the basin data as we only need the geometry and the identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read GRanD data into geodataframe\n",
    "grand_file_nm = 'data/var/GRanD_Version_1_3/GRanD_dams_v1_3.shp'\n",
    "grand_gdf = gpd.read_file(grand_file_nm)\n",
    "\n",
    "#Read in pickled level 12 HydroBASINS with geographic information (see step1_data_management.ipynb)\n",
    "gdf = f_mng.read_pkl_gdf()\n",
    "\n",
    "#subset basin data to only include identifiers and geometry\n",
    "gdf_sub = gdf[['HYBAS_ID','PFAF_ID','geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next use geopandas to spatially join the two datasets.  We use a left join from dams to basins to ensure each dam is assigned the basin it falls within\n",
    "* Note that using op='within' vs. op='intersection' save a lot of processing time (both gives same results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 116.36953754999558\n"
     ]
    }
   ],
   "source": [
    "start= timer()\n",
    "\n",
    "#Specifying within vs. intersection saves a lot of processing time\n",
    "spatial_join_gdf = sjoin(grand_gdf,gdf_sub,how='left',op='within')\n",
    "\n",
    "#Just for reference of how long takes to process\n",
    "proc_time = str(timer()-start) \n",
    "print(f'Processing time: {proc_time}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7320\n",
      "7320\n"
     ]
    }
   ],
   "source": [
    "# The new dataframe should have the same number of rows as the original dams geodataframe.\n",
    "print ((grand_gdf.shape)[0])\n",
    "print ((spatial_join_gdf.shape)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next summarize variables of interest.  In this case we are interested in count of dams and sum of max storage capacity.  If there is one dam in a basin the count = 1 and the storage capacity for that dam will carry over to the basin, but if multiple dams are in the same basin, we count the number of dams and sum the max capacities for all dams in the basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwief/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Select id columns and columns we would like to summarize\n",
    "summarize_prep = spatial_join_gdf[['HYBAS_ID','CAP_MCM']]\n",
    "#add count field that we can use to count dams per basin\n",
    "summarize_prep['count'] = 1\n",
    "#sum number of dams and max storage capacity of reservoirs for each basin with GRanD dams\n",
    "summarize_df = summarize_prep.groupby(['HYBAS_ID']).sum()\n",
    "summarize_df.reset_index(inplace=True)\n",
    "summarize_df = summarize_df.rename(columns={'count':'grand_dam_count','CAP_MCM':'grand_cap_mcm_sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HYBAS_ID', 'grand_cap_mcm_sum', 'grand_dam_count'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link back to original data and fill nan values with 0s (e.g. count of 0 dams for basins without data)\n",
    "all_basins_df = pd.DataFrame(gdf_sub.drop(columns=['geometry']))\n",
    "df_merge = all_basins_df.merge(summarize_df, how='left', on='HYBAS_ID')\n",
    "df_merge = df_merge.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1034083, 4)\n",
      "Index(['HYBAS_ID', 'PFAF_ID', 'grand_cap_mcm_sum', 'grand_dam_count'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (df_merge.shape)\n",
    "print (df_merge.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HYBAS_ID</th>\n",
       "      <th>PFAF_ID</th>\n",
       "      <th>grand_cap_mcm_sum</th>\n",
       "      <th>grand_dam_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1034080</th>\n",
       "      <td>2120113730</td>\n",
       "      <td>227405670002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034081</th>\n",
       "      <td>2120113740</td>\n",
       "      <td>214092090020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034082</th>\n",
       "      <td>2120113750</td>\n",
       "      <td>213097060014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           HYBAS_ID       PFAF_ID  grand_cap_mcm_sum  grand_dam_count\n",
       "1034080  2120113730  227405670002                0.0              0.0\n",
       "1034081  2120113740  214092090020                0.0              0.0\n",
       "1034082  2120113750  213097060014                0.0              0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to csv\n",
    "outfile_name = 'output/hb12_grand_local.csv'\n",
    "df_merge.to_csv(outfile_name, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring polygon and line to basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import file_management as f_mng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#any level can be used but in testing performance was best when ~ 3,000 lvl12 basins were processed at one time\n",
    "#these steps help ensure we are fairly close to the best performance\n",
    "#the average number of lvl12 basins in lvl 3 basins is , but the max is Y so we need to break those out further\n",
    "\n",
    "level = 3\n",
    "pfaf_id_list = f_mng.basin_list_by_pfaf_lvl(level=level)\n",
    "\n",
    "#read pickled geodataframe of lvl12 basins\n",
    "gdf = f_mng.read_pkl_gdf()\n",
    "gdf['PFAF_ID_str'] = gdf['PFAF_ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = 0\n",
    "under = 0\n",
    "for pfaf_id in pfaf_id_list:\n",
    "    pid = str(pfaf_id)\n",
    "    lvl3_gdf = gdf.loc[gdf['PFAF_ID_str'].str.startswith(pid,na=False)]\n",
    "    rows, cols = lvl3_gdf.shape\n",
    "    \n",
    "    if rows > 3000:\n",
    "        over+=1\n",
    "        #print (f'{pid} : {rows}')\n",
    "        #level = 5\n",
    "        #pfaf_id_list = f_mng.basin_list_by_pfaf_lvl(level=level)\n",
    "        #lvl5_list = [x for x in pfaf_id_list if str(x).startswith(pid)]\n",
    "        #for pfaf_id in lvl5_list:\n",
    "        #    pid5 = str(pfaf_id)\n",
    "        #    lvl5_gdf = lvl3_gdf.loc[lvl3_gdf['PFAF_ID_str'].str.startswith(pid5,na=False)]\n",
    "        #    rows, cols = lvl5_gdf.shape\n",
    "        #    if rows > 3000 or rows < 200:\n",
    "        #        print (f'{pid} : {rows}')\n",
    "    elif rows < 250:\n",
    "        under+=1\n",
    "        #print (f'{pid} : {rows}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b54943a371de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test['id']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
