{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains initial data management steps.  Functions related to this step can be found in the file_management.py module in utils -> 'utils/file_management.py' or 'from utils import file_management'\n",
    "\n",
    "Step 1a) This step documents data used in the project.  ScienceBase is used to document source data and retrieval metadata.  When permitted data were downloaded using Python code.  In some cases data needed to be requested and delivered. \n",
    "\n",
    "Step 1b) Documents data management steps and concepts to be considered before processing starts.\n",
    "\n",
    "Step 1c) Create file that documents information about Tiff files.  This file will be used to control processing methods.\n",
    "\n",
    "Step 2) Create serlialized versions of HydroSHEDS basin data for quick retrieval of information in processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Download Source Data\n",
    "      \n",
    "#### Standard HydroBASINS from HydroSHEDS: \n",
    "    * Store src HydroSHEDS data in repo under 'data/HydroSHEDS'\n",
    "    * Individual file names need to remain as downloaded\n",
    "    * Structure under this folder does not matter\n",
    "    * See more information and download directions here: Coming Soon\n",
    "    \n",
    "#### Landscape Variables (var files):\n",
    "    * Store src landscape variable (var) files in repo under 'data/var'\n",
    "    * Structure under this folder does not matter\n",
    "    * See more information and download directions for var files used in this assessment here: Coming Soon\n",
    "    \n",
    "#### Source File Information:\n",
    "    * User created csv file containing processing information about each var file\n",
    "    * For ease of use name and store file in the repo as: 'data/var/file_processing_info.csv'\n",
    "    * Required fields for each file include: file_name, variable, src_short, summary_type, label \n",
    "    * File name must match corresponsing var file name, shapefiles can be represented by only the .shp file\n",
    "    * See more information and download directions here: Coming Soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example showing a list of URLS with files for Land Cover data\n",
    "#Note was having trouble downloading these using Python, server kept cutting off access\n",
    "import urllib.request as url_r\n",
    "\n",
    "try:\n",
    "    target_url = 'https://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/manifest_cgls_lc_v2_100m_global_2015.txt'\n",
    "    download_urls = url_r.urlopen(target_url).read()\n",
    "    download_urls_str = download_urls.decode(\"utf8\")\n",
    "    list_urls = list(download_urls_str.split('\\r\\n'))\n",
    "except Exception as e:\n",
    "    raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E000N00_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip',\n",
       " 'https://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E000N20_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip',\n",
       " 'https://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E000N40_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows first 3 urls\n",
    "list_urls[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================================================================================\n",
    "******************************************************************************************************************\n",
    "=================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Create intermediate files containing data needed for processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Understand and modify initial data as needed\n",
    "    * Currently all source data were in the same coordinate system and no tests were built in yet.  User must ensure data are in {'init':'EPSG:4326'}, WGS84.\n",
    "    * The nodata value must be changed on some grids before the attribution process can be completed.  Not sure what is causing this, as the same nodata value is successfuly used by methods for some grids. Some example grids that needed this step include Pasture2000_5m.tif and Cropland2000_5m.tif.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster coordinate system EPSG:4326\n",
      "Shapefile coordinate system {'init': 'epsg:4326'}\n"
     ]
    }
   ],
   "source": [
    "#Example of how to verify coordinate system\n",
    "\n",
    "#for raster use rasterio to read data, then use crs method to get coordinate system information\n",
    "import rasterio\n",
    "file_name = 'data/var/CroplandPastureArea2000_Geotiff/Pasture2000_5m.tif'\n",
    "data = rasterio.open(file_name)\n",
    "coordinate_system = data.crs\n",
    "print (f'Raster coordinate system {coordinate_system}')\n",
    "\n",
    "#for shapefile use geopandas, read data in and call crs method, then use crs method to get coordinate system information\n",
    "import geopandas as gpd\n",
    "file_name = 'data/HydroSHEDS/basins/hybas_ar_lev12_v1c/hybas_ar_lev12_v1c.shp'\n",
    "gdf = gpd.read_file(file_name)\n",
    "gdf_coordinate_system = gdf.crs\n",
    "print (f'Shapefile coordinate system {gdf_coordinate_system}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of reassigning nodata value for the Pasture2000_5m grid\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "file = 'data/var/CroplandPastureArea2000_Geotiff/Pasture2000_5m.tif'\n",
    "data = rasterio.open(file)\n",
    "init_data = data.read()\n",
    "#In this case the initial nodata value is the only negative number, we replace that with the unique value of 999\n",
    "new_data = np.where(init_data<0, 999, init_data) \n",
    "\n",
    "#Use the profile from the orginal data but update the profile to have a nodata value of 999 and when writing\n",
    "#new file using the new_data where nodata is now represented as 999\n",
    "with rasterio.Env():\n",
    "    profile = data.profile\n",
    "    profile.update(nodata=999)\n",
    "    \n",
    "    #save new data to file 'data/var/CroplandPastureArea2000_Geotiff/Pasture2000_5m_update_nodata999.tif'\n",
    "    with rasterio.open('data/var/CroplandPastureArea2000_Geotiff/Pasture2000_5m_update_nodata999.tif', 'w', **profile) as dst:\n",
    "        dst.write(new_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================================================================================\n",
    "******************************************************************************************************************\n",
    "=================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1c) Create a file documenting information about landscape variable files that need processing.  This file provides information so methods know how to handle the files in summarization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Creates file: f'{directory}/{short_name}_ file_info.json'  -> example   data/var/tif_var_file_info.json\n",
    "    \n",
    "    * Example below creates json file  'data/var/tif_vars_file_info.json'.  The information in this file is used in later steps to inform how tif data from our source landscape variable datasets (stored in data/var directory in the repository) are attributed to basins in HydroSHEDS HYDROBASINS.  \n",
    "    \n",
    "    * Currently only gridded information (such as .tif) use this system, but hope to include similar process for vector data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwief/anaconda3/lib/python3.7/site-packages/rasterio/__init__.py:216: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/dwief/anaconda3/lib/python3.7/site-packages/rasterio/__init__.py:216: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from utils import file_management as f_mng\n",
    "\n",
    "directory = 'data/var'\n",
    "file_type = 'tif'\n",
    "extension = f'.{file_type}'\n",
    "\n",
    "short_name = f'{file_type}_vars'\n",
    "file_list, directory = f_mng.find_files(directory, suffix=extension)\n",
    "\n",
    "#convert file information to dictionary and export to json\n",
    "file_info, missing_info = f_mng.store_file_info(file_list, directory, short_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_name': 'W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_water-seasonal-coverfraction-layer_EPSG-4326.tif',\n",
       "  'file_path': 'data/var/LC100_epoch2015/W120N80_ProbaV_LC100_epoch2015_global_v2.0/W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_water-seasonal-coverfraction-layer_EPSG-4326.tif',\n",
       "  'bounds': {'xmin': -120.00049603174602,\n",
       "   'xmax': -100.0004960317461,\n",
       "   'ymin': 60.00049603174611,\n",
       "   'ymax': 80.00049603174604},\n",
       "  'no_data_val': 255.0,\n",
       "  'pixel_size': 0.0009920634920634888,\n",
       "  'crs': 'EPSG:4326',\n",
       "  'variable': 'water-seasonal-coverfraction-layer',\n",
       "  'src_short': 'lc100_epoch2015_v2.0.1',\n",
       "  'summary_type': 'count mean nodata',\n",
       "  'label': 'lc2015_waterseas_cov',\n",
       "  'categorical': 'no',\n",
       "  'pixel_inclusion': 'centroid'},\n",
       " {'file_name': 'W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_moss-coverfraction-layer_EPSG-4326.tif',\n",
       "  'file_path': 'data/var/LC100_epoch2015/W120N80_ProbaV_LC100_epoch2015_global_v2.0/W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_moss-coverfraction-layer_EPSG-4326.tif',\n",
       "  'bounds': {'xmin': -120.00049603174602,\n",
       "   'xmax': -100.0004960317461,\n",
       "   'ymin': 60.00049603174611,\n",
       "   'ymax': 80.00049603174604},\n",
       "  'no_data_val': 255.0,\n",
       "  'pixel_size': 0.0009920634920634888,\n",
       "  'crs': 'EPSG:4326',\n",
       "  'variable': 'moss-coverfraction-layer',\n",
       "  'src_short': 'lc100_epoch2015_v2.0.1',\n",
       "  'summary_type': 'count mean nodata',\n",
       "  'label': 'lc2015_moss_cov',\n",
       "  'categorical': 'no',\n",
       "  'pixel_inclusion': 'centroid'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show information about 2 files that were documented in the JSON\n",
    "file_info[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________\n",
    "#### Below is a count of the number of .tif files ready to be processed and a view of the information for the first 5 files.\n",
    "\n",
    "Note: File processing information is available in two formats.  The function passes back a list of dictionaries.  Each dictionary stores information about a file.  The same information is also saved to disk as 'data/var/tif_vars_file_info.json', for processing in later sessions.  Here is an example of printing the number of files processed and showing the first 5 records using the returned variable file_info.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940 files are documented and ready to process \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bounds</th>\n",
       "      <th>categorical</th>\n",
       "      <th>crs</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>no_data_val</th>\n",
       "      <th>pixel_inclusion</th>\n",
       "      <th>pixel_size</th>\n",
       "      <th>src_short</th>\n",
       "      <th>summary_type</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'xmin': -120.00049603174602, 'xmax': -100.000...</td>\n",
       "      <td>no</td>\n",
       "      <td>EPSG:4326</td>\n",
       "      <td>W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_w...</td>\n",
       "      <td>data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...</td>\n",
       "      <td>lc2015_waterseas_cov</td>\n",
       "      <td>255.0</td>\n",
       "      <td>centroid</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>lc100_epoch2015_v2.0.1</td>\n",
       "      <td>count mean nodata</td>\n",
       "      <td>water-seasonal-coverfraction-layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'xmin': -120.00049603174602, 'xmax': -100.000...</td>\n",
       "      <td>no</td>\n",
       "      <td>EPSG:4326</td>\n",
       "      <td>W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_m...</td>\n",
       "      <td>data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...</td>\n",
       "      <td>lc2015_moss_cov</td>\n",
       "      <td>255.0</td>\n",
       "      <td>centroid</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>lc100_epoch2015_v2.0.1</td>\n",
       "      <td>count mean nodata</td>\n",
       "      <td>moss-coverfraction-layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'xmin': -120.00049603174602, 'xmax': -100.000...</td>\n",
       "      <td>no</td>\n",
       "      <td>EPSG:4326</td>\n",
       "      <td>W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_c...</td>\n",
       "      <td>data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...</td>\n",
       "      <td>lc2015_crops_cov</td>\n",
       "      <td>255.0</td>\n",
       "      <td>centroid</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>lc100_epoch2015_v2.0.1</td>\n",
       "      <td>count mean nodata</td>\n",
       "      <td>crops-coverfraction-layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'xmin': -120.00049603174602, 'xmax': -100.000...</td>\n",
       "      <td>no</td>\n",
       "      <td>EPSG:4326</td>\n",
       "      <td>W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_t...</td>\n",
       "      <td>data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...</td>\n",
       "      <td>lc2015_tree_cov</td>\n",
       "      <td>255.0</td>\n",
       "      <td>centroid</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>lc100_epoch2015_v2.0.1</td>\n",
       "      <td>count mean nodata</td>\n",
       "      <td>tree-coverfraction-layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'xmin': -120.00049603174602, 'xmax': -100.000...</td>\n",
       "      <td>no</td>\n",
       "      <td>EPSG:4326</td>\n",
       "      <td>W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_g...</td>\n",
       "      <td>data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...</td>\n",
       "      <td>lc2015_grass_cov</td>\n",
       "      <td>255.0</td>\n",
       "      <td>centroid</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>lc100_epoch2015_v2.0.1</td>\n",
       "      <td>count mean nodata</td>\n",
       "      <td>grass-coverfraction-layer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              bounds categorical        crs  \\\n",
       "0  {'xmin': -120.00049603174602, 'xmax': -100.000...          no  EPSG:4326   \n",
       "1  {'xmin': -120.00049603174602, 'xmax': -100.000...          no  EPSG:4326   \n",
       "2  {'xmin': -120.00049603174602, 'xmax': -100.000...          no  EPSG:4326   \n",
       "3  {'xmin': -120.00049603174602, 'xmax': -100.000...          no  EPSG:4326   \n",
       "4  {'xmin': -120.00049603174602, 'xmax': -100.000...          no  EPSG:4326   \n",
       "\n",
       "                                           file_name  \\\n",
       "0  W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_w...   \n",
       "1  W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_m...   \n",
       "2  W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_c...   \n",
       "3  W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_t...   \n",
       "4  W120N80_ProbaV_LC100_epoch2015_global_v2.0.1_g...   \n",
       "\n",
       "                                           file_path                 label  \\\n",
       "0  data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...  lc2015_waterseas_cov   \n",
       "1  data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...       lc2015_moss_cov   \n",
       "2  data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...      lc2015_crops_cov   \n",
       "3  data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...       lc2015_tree_cov   \n",
       "4  data/var/LC100_epoch2015/W120N80_ProbaV_LC100_...      lc2015_grass_cov   \n",
       "\n",
       "   no_data_val pixel_inclusion  pixel_size               src_short  \\\n",
       "0        255.0        centroid    0.000992  lc100_epoch2015_v2.0.1   \n",
       "1        255.0        centroid    0.000992  lc100_epoch2015_v2.0.1   \n",
       "2        255.0        centroid    0.000992  lc100_epoch2015_v2.0.1   \n",
       "3        255.0        centroid    0.000992  lc100_epoch2015_v2.0.1   \n",
       "4        255.0        centroid    0.000992  lc100_epoch2015_v2.0.1   \n",
       "\n",
       "        summary_type                            variable  \n",
       "0  count mean nodata  water-seasonal-coverfraction-layer  \n",
       "1  count mean nodata            moss-coverfraction-layer  \n",
       "2  count mean nodata           crops-coverfraction-layer  \n",
       "3  count mean nodata            tree-coverfraction-layer  \n",
       "4  count mean nodata           grass-coverfraction-layer  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (f'{len(file_info)} files are documented and ready to process \\n')\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(file_info[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_name': 'WaterDepletionCat_WG3.tif',\n",
       "  'missing_fields': ['field_name',\n",
       "   'variable',\n",
       "   'src_short',\n",
       "   'summary_type',\n",
       "   'label',\n",
       "   'all_touched',\n",
       "   'conditional']},\n",
       " {'file_name': 'Pasture2000_5m_update_nodata999.tif',\n",
       "  'missing_fields': ['field_name',\n",
       "   'variable',\n",
       "   'src_short',\n",
       "   'summary_type',\n",
       "   'label',\n",
       "   'all_touched',\n",
       "   'conditional']},\n",
       " {'file_name': 'Cropland2000_5m.tif',\n",
       "  'missing_fields': ['field_name',\n",
       "   'variable',\n",
       "   'src_short',\n",
       "   'summary_type',\n",
       "   'label',\n",
       "   'all_touched',\n",
       "   'conditional']}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examples where files are in the directory but the file_processing_information.csv file does not include the files, shows 3 records \n",
    "missing_info[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________________________________________________________________\n",
    "#### The process also returns the variable 'missing_info', which is a list of dictionaries holding information about .tif files that were available and meeting the criteria (i.e. specified directory and file_type) yet were either not in the file_processing_info.csv file or were in the file but had missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 file(s) with missing fields in data/var/file_processing_info.csv\n",
      "3188 files(s) are not in data/var/file_processing_info.csv\n"
     ]
    }
   ],
   "source": [
    "#Example of how to use missing_info to understand what files that met criteria are not currently ready for processing\n",
    "\n",
    "#Import file_processing_info to get a count of number of columns\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/var/file_processing_info.csv')\n",
    "rows, cols = df.shape\n",
    "\n",
    "#If missing_info has data, count and print number of files that are not in the csv.\n",
    "#Also count and print number of files that are in the csv but have missing data.  In this case print the file name and missing fields.\n",
    "if missing_info:\n",
    "    w_missing_file = 0\n",
    "    w_missing_fields = 0\n",
    "    for record in missing_info:\n",
    "        if 'missing_fields' in record and len(record['missing_fields'])==cols:\n",
    "            w_missing_file += 1\n",
    "        elif 'missing_fields' in record and len(record['missing_fields'])<cols:\n",
    "            w_missing_fields += 1\n",
    "            print (f\"{record['file_name']} found in file_processing_info.csv but has missing fields: {record['missing_fields']}\")\n",
    "            print ('\\n')\n",
    "        elif 'file_failed' in record:\n",
    "            print (f\"{record['file_name']} failed\")\n",
    "            print ('\\n')\n",
    "    print (f'{w_missing_fields} file(s) with missing fields in data/var/file_processing_info.csv')\n",
    "    print (f'{w_missing_file} files(s) are not in data/var/file_processing_info.csv')\n",
    "else:\n",
    "    print('No files with missing processing information detected in processed directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================================================================================\n",
    "******************************************************************************************************************\n",
    "=================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Create serlialized versions of HydroSHEDS level 12 basins \n",
    "\n",
    "3 variations are created and pickled for future use allowing for quick access to consistent information throughout processing steps\n",
    "    * geodataframe containing all attributes and all geospatial information (poly)\n",
    "    * dataframe containing all attributes except geospatial information -> requires less RAM and loads faster when geospatial data are not needed\n",
    "    * list of hybas_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import file_management as f_mng\n",
    "f_mng.build_basin_data(level='12', version='v1c', directory = 'data/HydroSHEDS')\n",
    "#Prints number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HYBAS_ID</th>\n",
       "      <th>NEXT_DOWN</th>\n",
       "      <th>NEXT_SINK</th>\n",
       "      <th>MAIN_BAS</th>\n",
       "      <th>DIST_SINK</th>\n",
       "      <th>DIST_MAIN</th>\n",
       "      <th>SUB_AREA</th>\n",
       "      <th>UP_AREA</th>\n",
       "      <th>PFAF_ID</th>\n",
       "      <th>ENDO</th>\n",
       "      <th>COAST</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>SORT</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1120000010</td>\n",
       "      <td>0</td>\n",
       "      <td>1120000010</td>\n",
       "      <td>1120000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>111011001000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.50000000000002 29.94583333333337,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1120000020</td>\n",
       "      <td>0</td>\n",
       "      <td>1120000020</td>\n",
       "      <td>1120000020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>416.8</td>\n",
       "      <td>111011002100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((32.36250000000002 29.97083333333337,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HYBAS_ID  NEXT_DOWN   NEXT_SINK    MAIN_BAS  DIST_SINK  DIST_MAIN  \\\n",
       "0  1120000010          0  1120000010  1120000010        0.0        0.0   \n",
       "1  1120000020          0  1120000020  1120000020        0.0        0.0   \n",
       "\n",
       "   SUB_AREA  UP_AREA       PFAF_ID  ENDO  COAST  ORDER  SORT  \\\n",
       "0      11.0     11.0  111011001000     0      1      0     1   \n",
       "1     137.0    416.8  111011002100     0      0      1     2   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((32.50000000000002 29.94583333333337,...  \n",
       "1  POLYGON ((32.36250000000002 29.97083333333337,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test read gdf, show first 2 rows\n",
    "pkl_gdf = 'data/basins_lvl12_gdf.pkl'\n",
    "gdf = f_mng.read_pkl_gdf(pkl_gdf)\n",
    "gdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034083, 14)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There should be 1034083 rows and 14 columns\n",
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HYBAS_ID</th>\n",
       "      <th>NEXT_DOWN</th>\n",
       "      <th>NEXT_SINK</th>\n",
       "      <th>MAIN_BAS</th>\n",
       "      <th>DIST_SINK</th>\n",
       "      <th>DIST_MAIN</th>\n",
       "      <th>SUB_AREA</th>\n",
       "      <th>UP_AREA</th>\n",
       "      <th>PFAF_ID</th>\n",
       "      <th>ENDO</th>\n",
       "      <th>COAST</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>SORT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1120000010</td>\n",
       "      <td>0</td>\n",
       "      <td>1120000010</td>\n",
       "      <td>1120000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>111011001000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1120000020</td>\n",
       "      <td>0</td>\n",
       "      <td>1120000020</td>\n",
       "      <td>1120000020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>416.8</td>\n",
       "      <td>111011002100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HYBAS_ID  NEXT_DOWN   NEXT_SINK    MAIN_BAS  DIST_SINK  DIST_MAIN  \\\n",
       "0  1120000010          0  1120000010  1120000010        0.0        0.0   \n",
       "1  1120000020          0  1120000020  1120000020        0.0        0.0   \n",
       "\n",
       "   SUB_AREA  UP_AREA       PFAF_ID  ENDO  COAST  ORDER  SORT  \n",
       "0      11.0     11.0  111011001000     0      1      0     1  \n",
       "1     137.0    416.8  111011002100     0      0      1     2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test read df, show first 3 rows\n",
    "pkl_df_path = 'data/basins_lvl12_df.pkl'\n",
    "df=f_mng.read_pkl_df(pkl_df_path)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034083, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There should be 1034083 rows and 13 columns (same as gdf but no geometry)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1121976320, 4120903680, 3120562180, 8120172550, 2120220680]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test read list of HydroBASIN IDs, print first five\n",
    "pkl_df_path = 'data/basins_lvl12.txt'\n",
    "hybas_id_list=f_mng.read_pkl_df(pkl_df_path)\n",
    "hybas_id_list[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
