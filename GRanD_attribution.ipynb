{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code summarizes information about the GRanD dam dataset\n",
    "Specifically this code counts number of dams within each level 12 HydroBASIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed packages\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds taken for af: 52.21339047500078\n",
      "Seconds taken for ar: 11.36803035399862\n",
      "Seconds taken for as: 36.836592231000395\n",
      "Seconds taken for au: 20.20569632600018\n",
      "Seconds taken for eu: 32.19158324599994\n",
      "Seconds taken for gr: 4.369947941002465\n",
      "Seconds taken for na: 28.387256097001227\n",
      "Seconds taken for sa: 31.922468537002715\n",
      "Seconds taken for si: 23.20956921800098\n"
     ]
    }
   ],
   "source": [
    "#HydroBASINS regions to loop through\n",
    "regions = ['af','ar','as','au','eu','gr','na','sa','si']\n",
    "\n",
    "#Read GRanD data into geodataframe\n",
    "grand_file_nm = 'data/var/GRanD_Version_1_3/GRanD_dams_v1_3.shp'\n",
    "grand_gdf = gpd.read_file(grand_file_nm)\n",
    "\n",
    "#For each region do a spatial join to link GRanD dams to the basin\n",
    "appending_data = []\n",
    "for region in regions:\n",
    "    start= timer()\n",
    "    basin_file = f'data/HydroSHEDS/HydroBASINS/basins/hybas_{region}_lev12_v1c/hybas_{region}_lev12_v1c.shp'\n",
    "    gdf_basins = gpd.read_file(basin_file)\n",
    "    #Specifying within vs. intersection saves a lot of processing time\n",
    "    spatial_join_gdf = sjoin(grand_gdf,gdf_basins,how='left',op='within')\n",
    "    \n",
    "    #convert to pandas so we can concat regional dataframes (which isn't available in geopandas)\n",
    "    spatial_join_df = pd.DataFrame(spatial_join_gdf)\n",
    "    with_data = spatial_join_df.loc[spatial_join_df['HYBAS_ID'].notnull()]\n",
    "    \n",
    "    appending_data.append(with_data)\n",
    "    \n",
    "    #Just for reference of how long each region takes to process\n",
    "    proc_time = str(timer()-start) \n",
    "    print(f'Seconds taken for {region}: {proc_time}') \n",
    "    \n",
    "#Concatenate spatially joined data from each region into final global dataframe\n",
    "all_data = pd.concat(appending_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwief/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Select id columns and columns we would like to summarize\n",
    "summarize_prep = all_data[['PFAF_ID','HYBAS_ID','CAP_MCM']]\n",
    "#add count field that we can use to count dams per basin\n",
    "summarize_prep['count'] = 1\n",
    "#sum number of dams and max storage capacity of reservoirs for each basin with GRanD dams\n",
    "summarize_df = summarize_prep.groupby(['PFAF_ID','HYBAS_ID']).sum()\n",
    "summarize_df.reset_index(inplace=True)\n",
    "summarize_df = summarize_df.rename(columns={'count':'grand_dam_count','CAP_MCM':'grand_cap_mcm_sum','PFAF_ID':'pfaf_12'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to csv\n",
    "outfile_name = 'output/global_hb12_grand_stats.csv'\n",
    "summarize_df.to_csv(outfile_name, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dams accounted for!  YAY!!!\n"
     ]
    }
   ],
   "source": [
    "#Test to make sure all dams are accounted for.  Total dam count should equal number of dam records in initial GRanD geodataframe\n",
    "summarize_df['all']=1\n",
    "test_count = summarize_df.groupby(['all']).sum()\n",
    "num_grand_dams, num_col = grand_gdf.shape\n",
    "summary_count_of_dams = test_count.iloc[0]['grand_dam_count']\n",
    "if num_grand_dams == summary_count_of_dams:\n",
    "    print ('All dams accounted for!  YAY!!!')\n",
    "elif num_grand_dams < summary_count_of_dams:\n",
    "    num_dif = str(summary_count_of_dams-num_grand_dams)\n",
    "    print (f'Count is higher than number of dams in GRanD. {num_dif} to many dams in count.')\n",
    "elif num_grand_dams > summary_count_of_dams:\n",
    "    num_dif = str(num_grand_dams-summary_count_of_dams)\n",
    "    print (f'Count is lower than number of dams in GRanD. {num_dif} dams not accounted for.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to make sure there is a 1:1 relation between HYBAS_ID and PFAF_ID in level 12 basins\n",
    "for region in regions:\n",
    "    start= timer()\n",
    "    basin_file = f'data/HydroSHEDS/HydroBASINS/basins/hybas_{region}_lev12_v1c/hybas_{region}_lev12_v1c.shp'\n",
    "    gdf_basins = gpd.read_file(basin_file)\n",
    "    test =gdf_basins[['HYBAS_ID','PFAF_ID']]\n",
    "    print(any(summarize_df['HYBAS_ID'].duplicated()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
